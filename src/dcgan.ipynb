{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pablo Vicente Juan, Ming Zhou and Macrina Mar√≠a Lobo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pv2288, mz2591 and mml2204"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Representation Learning With Deep Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import timeit\n",
    "import numpy as np\n",
    "from random import Random\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "\n",
    "from helper import data_augmenter\n",
    "from helper.metrics import nn_score\n",
    "from helper.optimizer import SGD, Adam\n",
    "from helper.operator import batchnorm, deconv\n",
    "from helper.initialization import normal, constant\n",
    "from helper.data_augmenter import translate, rotate, flip, blur\n",
    "from helper.network_builder import build_discriminator, build_generator\n",
    "from helper.image_manipulation import transform, inverse_transform, generate_samples\n",
    "from helper.data_handler import load_dataset, concatenate_datasets, create_dataset_from_folder\n",
    "\n",
    "relu = T.nnet.relu\n",
    "tanh = T.tanh\n",
    "\n",
    "binary_crossentropy = T.nnet.binary_crossentropy\n",
    "\n",
    "theano.config.floatX = 'float32'\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "f = sys.stdout# open('log.txt','w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimizer parameters\n",
    "l2_reg = 1e-5               # l2_reg weight decay\n",
    "learning_rate = 0.0002      # initial learning rate for adam\n",
    "momentum = 0.5              # momentum term of adam\n",
    "\n",
    "# Image dimension\n",
    "n_channels = 3              # Number of channels in image\n",
    "img_size = 64               # Number of pixels width/height of images\n",
    "\n",
    "# Training parameters\n",
    "n_gen_samples = 196         # Number of samples to save during\n",
    "n_epochs = 1000             # Number of epochs\n",
    "batch_size = 128            # Number of examples in batch\n",
    "epoch_results = 4           # Frequency to save the intermediate results\n",
    "epoch_params = 20           # Frequency to save the network parameters\n",
    "\n",
    "#Architecture parameters\n",
    "n_g_filters = 128           # Number of generator filters in first conv layer\n",
    "n_d_filters = 128           # Number of discriminator filters in first conv layer\n",
    "dimZ = 100                  # Number of dim for Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Theano variable for real images\n",
    "X = T.tensor4('X')\n",
    "# Theano variable for random noise vector\n",
    "Z = T.matrix('Z')\n",
    "\n",
    "# Generator architecture\n",
    "g_initial_im_size = 4\n",
    "g_flat_size = (dimZ, n_g_filters*8*4*4)\n",
    "g_layer_size = [n_g_filters*4, n_g_filters*2, n_d_filters, n_channels]\n",
    "g_num_filters = [n_g_filters*8, n_g_filters*4, n_d_filters*2, n_d_filters]\n",
    "g_filter_size = [5, 5, 5, 5]\n",
    "g_norm = [True, True, True, False]\n",
    "g_activation = [relu, relu, relu, tanh]\n",
    "g_subsample = [(2,2),(2,2),(2,2),(2,2)]\n",
    "g_border_mode = [(2,2),(2,2),(2,2),(2,2)]\n",
    "\n",
    "# Discriminator architecture\n",
    "d_layer_size = [n_channels, n_d_filters, n_d_filters*2, n_d_filters*4]\n",
    "d_num_filters = [n_d_filters, n_d_filters*2, n_d_filters*4, n_d_filters*8]\n",
    "d_filter_size = [5, 5, 5, 5]\n",
    "d_norm = [False, True, True, True]\n",
    "d_flat_size = (n_d_filters*8*4*4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load training and testing dataset and combine them to have a bigger sample\n",
    "cars, n_train_batches = load_dataset('../data/cars.npy', batch_size)\n",
    "cars2, n_train_batches = load_dataset('../data/cars_test.npy', batch_size)\n",
    "datasetX, n_train_batches = concatenate_datasets((cars, cars2), batch_size, axis=0)\n",
    "datasetX = transform(datasetX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The same random vector is used each time to generate the samples stored\n",
    "# The reason to do this lies on being able to see the evolution from a set of inputs over time\n",
    "sample_randomZ = np.asarray(rng.uniform(-1., 1., size=(n_gen_samples, dimZ)), dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build generator\n",
    "gX, g_layers = build_generator(Z, g_layer_size, g_num_filters, g_filter_size, g_flat_size, \n",
    "                               g_subsample, g_border_mode, g_norm, g_activation, g_initial_im_size)\n",
    "\n",
    "# List of generator parameters to be optimized\n",
    "g_params = [item for sublist in [layer.params for layer in g_layers] for item in sublist]\n",
    "\n",
    "# Build discriminator for real samples\n",
    "p_real, d_real_layers = build_discriminator(X, d_layer_size, d_num_filters, d_filter_size, \n",
    "                                            d_flat_size, d_norm)\n",
    "\n",
    "# List of discriminator parameters to be optimized\n",
    "d_params = [item for sublist in [layer.params for layer in d_real_layers] for item in sublist]\n",
    "\n",
    "# Build discriminator for generated samples, it is the same as the previous discriminator\n",
    "# They are duplicate to facilitate creating the assignid the label for the real and false\n",
    "# samples. \n",
    "p_gen, d_gen_layers = build_discriminator(gX, d_layer_size, d_num_filters, d_filter_size, \n",
    "                                          d_flat_size, d_norm, d_params)\n",
    "\n",
    "# Cost to be optimized\n",
    "# The cost of the generator is the sum of the loss of not predicting the real images as one\n",
    "# plus the loss of not predicting the false images as zeros. The cost of the generator \n",
    "# is the loss of not being able to fool the discriminator\n",
    "d_cost_real = binary_crossentropy(p_real, T.ones(p_real.shape)).mean()\n",
    "d_cost_gen = binary_crossentropy(p_gen, T.zeros(p_gen.shape)).mean()\n",
    "g_cost_d = binary_crossentropy(p_gen, T.ones(p_gen.shape)).mean()\n",
    "\n",
    "d_cost = d_cost_real + d_cost_gen\n",
    "g_cost = g_cost_d\n",
    "\n",
    "lr_shared = theano.shared(np.asarray(learning_rate, dtype=theano.config.floatX))\n",
    "d_updater = Adam(lr=lr_shared, b1=momentum, l2=l2_reg)\n",
    "g_updater = Adam(lr=lr_shared, b1=momentum, l2=l2_reg)\n",
    "\n",
    "d_updates = d_updater(d_params, d_cost)\n",
    "g_updates = g_updater(g_params, g_cost)\n",
    "updates = d_updates + g_updates\n",
    "\n",
    "# The theano function using the defines cost and the update procedure are created\n",
    "_train_g = theano.function([Z], g_cost, updates=g_updates)\n",
    "_train_d = theano.function([X, Z], d_cost, updates=d_updates)\n",
    "_gen = theano.function([Z], gX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Only one pass over the entire dataset for the discriminator\n",
    "# is done in this dataset before training both networks\n",
    "for minibatch_index in range(n_train_batches):\n",
    "\n",
    "    x_batch = datasetX[minibatch_index * batch_size: (minibatch_index + 1) * batch_size]        \n",
    "    randomZ = np.asarray(rng.uniform(-1., 1., size=(len(x_batch), dimZ)), dtype=theano.config.floatX)\n",
    "    d_loss = _train_d(x_batch, randomZ)\n",
    "    \n",
    "print(('d_cost %.4f') % (d_loss), file=f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epoch = 0    \n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "while (epoch < n_epochs):\n",
    "    \n",
    "    # Shuffle samples at the beginning of each epoch\n",
    "    np.random.shuffle(datasetX)\n",
    "    \n",
    "    # For each minibatch on the training set\n",
    "    for minibatch_index in range(3):#n_train_batches):\n",
    "\n",
    "        # A minibath is retrived and a random vector Z is sampled\n",
    "        x_batch = datasetX[minibatch_index * batch_size: (minibatch_index + 1) * batch_size]        \n",
    "        randomZ = np.asarray(rng.uniform(-1., 1., size=(len(x_batch), dimZ)), dtype=theano.config.floatX)\n",
    "             \n",
    "        # The generator is trained in the first place \n",
    "        g_loss = _train_g(randomZ)   \n",
    "        \n",
    "        # Afterwards, the discriminator is trained on a new random vector\n",
    "        randomZ = np.asarray(rng.uniform(-1., 1., size=(len(x_batch), dimZ)), dtype=theano.config.floatX)\n",
    "        d_loss = _train_d(x_batch, randomZ)   \n",
    "        \n",
    "        # Only use in some test cases where the cost of the generator diverges quickly\n",
    "        #if (g_loss-1.5) > d_loss:\n",
    "        #    randomZ = np.asarray(rng.uniform(-1., 1., size=(len(x_batch), dimZ)), dtype=theano.config.floatX)\n",
    "        #    g_loss = _train_g(randomZ)        \n",
    "            \n",
    "    print(('epoch %d g_loss %.4f d_loss %.4f') % (epoch, g_loss, d_loss), file=f)    \n",
    "    f.flush()    \n",
    "\n",
    "    # Validation metric is calculated by obtained the distance to the closet real\n",
    "    # sample of the generated ones using 1 Nearest Neighbour\n",
    "    if epoch % epoch_results == 0:\n",
    "        gX = generate_samples(_gen, n_batches=1, batch_size=batch_size, size_Z=dimZ)\n",
    "        gX = gX.reshape(len(gX), -1)\n",
    "\n",
    "        validation = nn_score(gX, x_batch.reshape(len(x_batch), -1))    \n",
    "        print(('validation %.2f g_loss %.4f d_loss %.4f') % (validation, g_loss, d_loss), file=f)    \n",
    "        f.flush()\n",
    "\n",
    "        np.save('../results/new_generated_images' + str(epoch) + '.npy', _gen(sample_randomZ))  \n",
    "        \n",
    "    if epoch % epoch_params == 0:\n",
    "        pickle.dump(d_params, open('../results/d_params.p', 'wb'))\n",
    "        pickle.dump(g_params, open('../results/g_params.p', 'wb'))\n",
    "        pickle.dump(sample_randomZ, open('../results/sample_randomZ.p', 'wb'))\n",
    "\n",
    "    epoch = epoch + 1    \n",
    "    \n",
    "pickle.dump(d_params, open( \"d_params.p\", \"wb\" ))\n",
    "pickle.dump(g_params, open( \"g_params.p\", \"wb\" ))\n",
    "pickle.dump(sample_randomZ, open( \"sample_randomZ.p\", \"wb\" ))\n",
    "\n",
    "print(('The code ran for %.2fm' % ((end_time - start_time) / 60.)), file=f)    \n",
    "f.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
